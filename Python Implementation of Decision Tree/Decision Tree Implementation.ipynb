{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cognitive-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "congressional-occasion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\Users\\\\ashut\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-monroe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all_features=np.array([i for i in df.columns]) , change the name '(from this df.columns to somethig like [name].columns )'\\n   of dataframe in the code , which is present in the implementation section of the decisionn tree \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Define name of the dataFrame as 'df' as this name would be used later in the decision tree . if you want to provide\n",
    "   different name for the dataframe, please also chage the name of the same in the implementation part of decisio tree\"\"\"\n",
    "\n",
    "\"\"\"all_features=np.array([i for i in df.columns]) , change the name '(from this df.columns to somethig like [name].columns )'\n",
    "   of dataframe in the code , which is present in the implementation section of the decisionn tree \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "familiar-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "military-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "header= ['sl','sw','pl','pw']\n",
    "df.columns=header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ruled-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting numeric values to a distinct/ labels \n",
    "def label(val,*boundary):\n",
    "    if val<boundary[0]:\n",
    "        return 'a'\n",
    "    elif val<boundary[1]:\n",
    "        return 'b'\n",
    "    elif val<boundary[2]:\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hairy-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying these labels to the dataset\n",
    "def convert(df,oldFeatures):\n",
    "    second = df[oldFeatures].mean()\n",
    "    minimum = df[oldFeatures].min()\n",
    "    first = (minimum+second)/2\n",
    "    maximum = df[oldFeatures].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[oldFeatures].apply(label,args=(first,second,third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suburban-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting the dataframe\n",
    "df['SL']=convert(df,'sl')\n",
    "df['SW']=convert(df,'sw')\n",
    "df['PL']=convert(df,'pl')\n",
    "df['PW']=convert(df,'pw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metropolitan-guarantee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SL</th>\n",
       "      <th>SW</th>\n",
       "      <th>PL</th>\n",
       "      <th>PW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SL SW PL PW\n",
       "0  b  c  a  a\n",
       "1  a  b  a  a\n",
       "2  a  c  a  a\n",
       "3  a  c  a  a\n",
       "4  a  c  a  a"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['sl','sw','pl','pw'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "improving-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for our decision tree we do the following:\\nwe start with a node; \\ncase 1 = if the node is pure we dont need to split any further and output would be the only class present\\ncase 2 = if there is no feature left to split upon , we provide the majority among the class as the output\\nIf both the above condition are not met , then we find the best splitting condition and recursivelly call the split'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for our decision tree we do the following:\n",
    "we start with a node; \n",
    "case 1 = if the node is pure we dont need to split any further and output would be the only class present\n",
    "case 2 = if there is no feature left to split upon , we provide the majority among the class as the output\n",
    "If both the above condition are not met , then we find the best splitting condition and recursivelly call the split'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "informational-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data,output):\n",
    "#        Data = 1)it is the feature on which our node is going to split upon when fitting the training data.\n",
    "#               2)its values is none for leaf node\n",
    "#        Index = For assigning a preticular index to each node\n",
    "#        Output = it represents the class with majority at that instance\n",
    "#        Children = it is storing the children of node in the form of a dictionary for easy access\n",
    "        self.data = data\n",
    "        self.children = {}\n",
    "        self.output = output\n",
    "        self.index = -1\n",
    "   \n",
    "    def add_child(self,feature_value,obj):\n",
    "#       adding values of children in a dictionary form \n",
    "        self.children[feature_value] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "lovely-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_tree:\n",
    "    def __init__(self):\n",
    "        # this function provide the root node of our decision tree\n",
    "        self.__root = None\n",
    "\n",
    "    def frequency_counter(self,Y):\n",
    "        # return the number of counts of unique classes in the dataset in the form of a dictionary\n",
    "        count = {}\n",
    "        for i in Y:\n",
    "            if i in count:\n",
    "                count[i]+=1\n",
    "            else:\n",
    "                count[i]=1\n",
    "        return count\n",
    "\n",
    "\n",
    "    def entropy(self,Y):\n",
    "        # this function provides us with the entropy value of a given node in our tree\n",
    "        freq = self.frequency_counter(Y)\n",
    "        ent = 0\n",
    "        total = len(Y)\n",
    "        for i in freq:\n",
    "            p = freq[i]/total\n",
    "            ent += (-p)*math.log(p)\n",
    "        return ent\n",
    "\n",
    "    def gain_ratio(self,X,Y,selected_feature):\n",
    "        # this function provides us with the gain ratio which is used as a metric for getting a better accuracy for out decision tree\n",
    "        infoOriginal = self.entropy(Y) # \"original\" represents entropy before splitting\n",
    "        infoF = 0 \n",
    "        split_info = 0\n",
    "        values = set(X[:,selected_feature])\n",
    "        df = pd.DataFrame(X)\n",
    "        df[df.shape[1]] = Y# Appending Y values as the last column in the newely created dataframe \n",
    "        starting_size = df.shape[0] \n",
    "        for i in values:\n",
    "            df1 = df[df[selected_feature] == i]\n",
    "            current_size = df1.shape[0]\n",
    "            infoF += (current_size/starting_size)*self.entropy(df1[df1.shape[1]-1])\n",
    "            split_info += (-current_size/starting_size)*math.log(current_size/starting_size)\n",
    "\n",
    "        # handling zero value error in case of splitInfo\n",
    "        if split_info == 0 :\n",
    "            return math.inf \n",
    "    \n",
    "\n",
    "        info_gain = infoOriginal - infoF\n",
    "        gain_ratio = info_gain / split_info\n",
    "        return gain_ratio \n",
    "\n",
    "\n",
    "\n",
    "    def decision_tree(self,X,Y,features,level,classes, all_features=np.array([i for i in df.columns])):\n",
    "#       classes = different class available for us based on dataset\n",
    "#       level = represent depth of out tree\n",
    "#       all_features = provides all the output labesl present in the dataframe\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "#       If the node consists of a single pure class\n",
    "        if len(set(Y)) == 1:\n",
    "            print(\"Level\",level)\n",
    "            output = None\n",
    "            for i in classes:\n",
    "                if i in Y:\n",
    "                    output = i\n",
    "                    print(\"Count of\",i,\"=\",len(Y))\n",
    "                else :\n",
    "                    print(\"Count of\",i,\"=\",0)\n",
    "            print(\"Current Entropy is =  0.0\")\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "\n",
    "#       If we run out of features to split upon\n",
    "#       In this case the answer will be the class whose count is maximum for the given split\n",
    "        if len(features) == 0:\n",
    "            print(\"Level\",level)\n",
    "            freqs = self.frequency_counter(Y)\n",
    "            output = None\n",
    "            max_count = -math.inf\n",
    "            for i in classes:\n",
    "                if i not in freqs:\n",
    "                    print(\"Count of\",i,\"=\",0)\n",
    "                else :\n",
    "                    if freqs[i] > max_count :\n",
    "                        output = i\n",
    "                        max_count = freqs[i]\n",
    "                    print(\"Count of\",i,\"=\",freqs[i])\n",
    "\n",
    "            print(\"Current Entropy  is =\",self.entropy(Y))          \n",
    "\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "\n",
    "        \n",
    "        \n",
    "        # after the two base cases, we have to Find the best feature to split upon\n",
    "        max_gain = -math.inf\n",
    "        final_feature = None\n",
    "        for f in features :\n",
    "            current_gain = self.gain_ratio(X,Y,f)\n",
    "            if current_gain > max_gain:\n",
    "                max_gain = current_gain\n",
    "                final_feature = f\n",
    "\n",
    "        print(\"Level\",level)\n",
    "        freqs = self.frequency_counter(Y)\n",
    "        output = None\n",
    "        max_count = -math.inf\n",
    "        for i in classes:\n",
    "            if i not in freqs:\n",
    "                print(\"Count of\",i,\"=\",0)\n",
    "            else :\n",
    "                if freqs[i] > max_count :\n",
    "                    output = i\n",
    "                    max_count = freqs[i]\n",
    "                print(\"Count of\",i,\"=\",freqs[i])     \n",
    "        print(\"Current Entropy is =\",self.entropy(Y))\n",
    "        print(\"Splitting on feature\" , all_features[final_feature] , \"with gain ratio\" , max_gain)\n",
    "        print()\n",
    "        \n",
    "\n",
    "            \n",
    "        unique_values = set(X[:,final_feature]) # unique_values represents the unique values of the feature selected\n",
    "        df = pd.DataFrame(X)        \n",
    "        df[df.shape[1]] = Y# appending y values at the end of the data frame.\n",
    "        current_node = TreeNode(final_feature,output)\n",
    "        index  = features.index(final_feature)\n",
    "        features.remove(final_feature)# removing the selected feature from the list.\n",
    "        for i in unique_values:\n",
    "            # Creating a new dataframe with value of selected feature = i\n",
    "            df_new = df[df[final_feature] == i]\n",
    "            # recursively calling on the splits\n",
    "            node = self.decision_tree(df_new.iloc[:,0:df_new.shape[1]-1].values,df_new.iloc[:,df_new.shape[1]-1].values,features,level+1,classes)\n",
    "            current_node.add_child(i,node)\n",
    "\n",
    "        # Add the removed feature     \n",
    "        features.insert(index,final_feature)\n",
    "\n",
    "        return current_node\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        # Fits according to the given training data\n",
    "        features = [i for i in range(len(X[0]))]\n",
    "        classes = set(Y)\n",
    "        level = 0\n",
    "        self.root = self.decision_tree(X,Y,features,level,classes)\n",
    "        \n",
    "    def predictSingle(self,data,node):\n",
    "        # predicts the class for a given testing point\n",
    "        # when We reach, a leaf node\n",
    "        if len(node.children) == 0 :\n",
    "            return node.output\n",
    "        val = data[node.data] # represents the value of feature on which the split is made       \n",
    "        if val not in node.children :\n",
    "            return node.output\n",
    "        # Recursively call on the splits\n",
    "        return self.predictSingle(data,node.children[val])\n",
    "\n",
    "    def predict(self,X):\n",
    "        # This function returns Y-predicted\n",
    "        Y = np.array([0 for i in range(len(X))])\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = self.predictSingle(X[i],self.root)\n",
    "        return Y\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        # this function returns the mean accuracy.\n",
    "        Y_predicted = self.predict(X)\n",
    "        counter = 0\n",
    "        for i in range(len(Y_predicted)):\n",
    "            if Y_predicted[i] == Y[i]:\n",
    "                counter+=1\n",
    "        return counter/len(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "committed-employment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 42\n",
      "Count of 1 = 31\n",
      "Count of 2 = 39\n",
      "Current Entropy is = 1.0906896614201194\n",
      "Splitting on feature PW with gain ratio 0.7227537963335781\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 27\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 26\n",
      "Count of 2 = 12\n",
      "Current Entropy is = 0.6236548495681085\n",
      "Splitting on feature PL with gain ratio 0.40276291385112956\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 6\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 25\n",
      "Count of 2 = 6\n",
      "Current Entropy is = 0.4913274484505221\n",
      "Splitting on feature SL with gain ratio 0.1397212077667352\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 15\n",
      "Count of 2 = 5\n",
      "Current Entropy is = 0.5623351446188083\n",
      "Splitting on feature SW with gain ratio 0.11009755776163734\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 6\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 3\n",
      "Count of 2 = 1\n",
      "Current Entropy  is = 0.5623351446188083\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 6\n",
      "Count of 2 = 4\n",
      "Current Entropy  is = 0.6730116670092565\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 8\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 42\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 5\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x= df.values\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test=train_test_split(x, y)\n",
    "clf=build_tree()\n",
    "clf.fit(X_train, y_train)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "guided-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score= 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "print(\"Score=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "nearby-belief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction on training data= [2 0 0 0 2 2 2 2 1 1 2 1 2 1 1 0 0 1 0 0 0 1 0 1 0 0 0 2 1 1 0 0 2 0 0 0 0\n",
      " 2 2 1 1 0 2 2 1 1 2 2 1 0 1 1 1 1 1 2 0 0 2 1 0 0 0 2 0 0 2 1 2 1 0 2 1 1\n",
      " 2 0 1 2 0 2 1 2 1 2 2 2 2 1 0 0 0 0 0 1 1 2 1 0 2 2 0 2 1 0 2 1 0 0 1 1 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print('prediction on training data=' , clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "immediate-carter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction on testing data= [1 0 2 2 2 1 1 0 2 1 1 2 0 1 2 1 2 0 1 1 1 1 1 0 1 1 1 1 2 1 0 2 0 1 1 1 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print('prediction on testing data=', clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-seeking",
   "metadata": {},
   "source": [
    "## Implementation with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "found-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "neither-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "connected-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain,XTest,yTrain,yTest = train_test_split(iris.data,iris.target,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vocal-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(XTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spatial-ranch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 2, 0, 1, 2, 1, 0, 0, 1,\n",
       "       0, 1, 2, 2, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 0, 0, 0, 1, 0, 0, 0, 2,\n",
       "       0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 1, 0, 2, 2, 0,\n",
       "       2, 1, 1, 2, 0, 2, 0, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 2, 2, 1, 0, 0,\n",
       "       2, 1, 0, 1, 2, 2, 2, 1, 0, 2, 0, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 1,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predTrain = algo.predict(XTrain)\n",
    "predTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vocal-routine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 2, 1, 1, 2, 0, 2, 0, 0, 2, 2, 1, 1, 1, 0, 2, 1, 0,\n",
       "       1, 1, 1, 1, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predTest =algo.predict(XTest)\n",
    "predTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entitled-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "associate-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  0  0]\n",
      " [ 0 33  0]\n",
      " [ 0  0 39]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(yTrain,predTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "radio-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0 17  0]\n",
      " [ 0  1 10]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(yTest,predTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "exposed-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "experienced-venue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(yTest,predTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "minimal-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accepted-station",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data=export_graphviz(algo,out_file=None,feature_names=iris.feature_names,class_names =iris.target_names)\n",
    "graph =pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf('iris.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "statutory-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are getting above 90 % accuracy with both the models, which is pretty good scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "enhanced-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in the preprocessing of the data , if we code our function like following:\n",
    "# def convert(df,oldFeatures):\n",
    "#     second = df[oldFeatures].mean()\n",
    "#     first = 0.5 * second\n",
    "#     third = 1.5 * second\n",
    "#     return df[oldFeatures].apply(label,args=(first,second,third))\n",
    "\n",
    "# in this case we would be getting accuracy around '0.8947368421052632' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
